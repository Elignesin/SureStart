{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "March2_EmotionDetection/Vision.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1716CCsLLr088E2ncee6G1kgIjmnx-2Od",
      "authorship_tag": "ABX9TyMD6yllyo29Nq5558UJYlB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Elignesin/SureStart/blob/main/March2_EmotionDetection_Vision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SprNZO78EH9q"
      },
      "source": [
        "### Emotion Detection and Computer Vision\r\n",
        "\r\n",
        "The purpose of this notebook is to learn how to detect emotions using Neural Networks. This notebook will follow the tutorial found at https://medium.com/swlh/emotion-detection-using-opencv-and-keras-771260bbd7f7 for emotion detection using Keras and OpenCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDpj3XrSEDni"
      },
      "source": [
        "#Import the necessary libraries\r\n",
        "import keras\r\n",
        "from keras.preprocessing.image import ImageDataGenerator\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\r\n",
        "from keras.layers import Conv2D,MaxPooling2D\r\n",
        "from keras.optimizers import RMSprop,SGD,Adam\r\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\r\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT202olbIPDJ"
      },
      "source": [
        "#Now for some definitions we need for later\r\n",
        "num_classes=5\r\n",
        "img_rows,img_cols=48,48\r\n",
        "batch_size=32"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qadqtkVQONBr",
        "outputId": "b247648f-e393-4aca-b7c4-1ec2b026401b"
      },
      "source": [
        "os.chdir('./drive/MyDrive/Colab Notebooks/')\r\n",
        "os.getcwd()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab Notebooks'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5z87jxbkJK5Z"
      },
      "source": [
        "#Now we need to download the dataset\r\n",
        "#The dataset comes from https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\r\n",
        "#I took the file from the google drive from the github of the article writer above\r\n",
        "train_data_dir='fer2013/train'\r\n",
        "validation_data_dir='fer2013/validation'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlLEpiLBLJo8"
      },
      "source": [
        "#Create image generators for image augmentation and rescaling for both train and validation datasets\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./255,\r\n",
        "    rotation_range=30,\r\n",
        "    shear_range=0.3,\r\n",
        "    zoom_range=0.3,\r\n",
        "    width_shift_range=0.4,\r\n",
        "    height_shift_range=0.4,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest')\r\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIulAURqLREU",
        "outputId": "8765609a-604d-45e5-d029-372f9852827a"
      },
      "source": [
        "#Apply the train and validation generators to the directories to read in our datasets and augment and rescale them\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        " train_data_dir,\r\n",
        " color_mode='grayscale',\r\n",
        " target_size=(img_rows,img_cols),\r\n",
        " batch_size=batch_size,\r\n",
        " class_mode='categorical',\r\n",
        " shuffle=True)\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        " validation_data_dir,\r\n",
        " color_mode='grayscale',\r\n",
        " target_size=(img_rows,img_cols),\r\n",
        " batch_size=batch_size,\r\n",
        " class_mode='categorical',\r\n",
        " shuffle=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 24256 images belonging to 5 classes.\n",
            "Found 3006 images belonging to 5 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAm5dKEUMRA7",
        "outputId": "65225841-a2b7-43b5-f4e5-89fa0380b7b4"
      },
      "source": [
        "#We're going to build a basic CNN using Keras Sequential models\r\n",
        "#The article uses 7 different blocks in the model, I'm going to cut this to 4 \r\n",
        "model = Sequential()\r\n",
        "#Block-1\r\n",
        "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\r\n",
        "                 input_shape=(img_rows,img_cols,1)))\r\n",
        "model.add(Activation('elu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\r\n",
        "                 input_shape=(img_rows,img_cols,1)))\r\n",
        "model.add(Activation('elu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "model.add(Dropout(0.2))\r\n",
        "#Block-2\r\n",
        "#model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
        "#model.add(Activation('elu'))\r\n",
        "#model.add(BatchNormalization())\r\n",
        "#model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\r\n",
        "#model.add(Activation('elu'))\r\n",
        "#model.add(BatchNormalization())\r\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\r\n",
        "#model.add(Dropout(0.2))\r\n",
        "#Block-5\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(8,kernel_initializer='he_normal')) #I cut this from 64 units to 16 to reduce time\r\n",
        "model.add(Activation('elu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.5))\r\n",
        "#Block-6\r\n",
        "model.add(Dense(8,kernel_initializer='he_normal')) #I cut this from 64 units to 32\r\n",
        "model.add(Activation('elu'))\r\n",
        "model.add(BatchNormalization())\r\n",
        "model.add(Dropout(0.5))\r\n",
        "#Block-7\r\n",
        "model.add(Dense(num_classes,kernel_initializer='he_normal'))\r\n",
        "model.add(Activation('softmax'))\r\n",
        "#Show a summary of the model\r\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 8)                 147464    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 5)                 45        \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 157,469\n",
            "Trainable params: 157,309\n",
            "Non-trainable params: 160\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CctrnwnVM8Lr"
      },
      "source": [
        "#Create some callbacks for use during training\r\n",
        "#This came straight from the article, I didn't change anything here\r\n",
        "checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\r\n",
        "                             monitor='val_loss',\r\n",
        "                             mode='min',\r\n",
        "                             save_best_only=True,\r\n",
        "                             verbose=1)\r\n",
        "earlystop = EarlyStopping(monitor='val_loss',\r\n",
        "                          min_delta=0,\r\n",
        "                          patience=3,\r\n",
        "                          verbose=1,\r\n",
        "                          restore_best_weights=True\r\n",
        "                          )\r\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\r\n",
        "                              factor=0.2,\r\n",
        "                              patience=3,\r\n",
        "                              verbose=1,\r\n",
        "                              min_delta=0.0001)\r\n",
        "callbacks = [earlystop,checkpoint,reduce_lr]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr0sHbcINawo",
        "outputId": "a1f12521-baf3-4d80-fb57-bda6a283c097"
      },
      "source": [
        "#Now, we'll compile and fit the model\r\n",
        "opt = Adam()\r\n",
        "model.compile(optimizer = opt, loss='categorical_crossentropy', metrics=['accuracy'])\r\n",
        "\r\n",
        "#Now fit the model with fit\r\n",
        "nb_train_samples = 24176\r\n",
        "nb_validation_samples = 3006\r\n",
        "epochs=5 #I cut this from 25 to 5 to save time\r\n",
        "history=model.fit(\r\n",
        " train_generator,\r\n",
        " steps_per_epoch=nb_train_samples//batch_size,\r\n",
        " epochs=epochs,\r\n",
        " callbacks=callbacks,\r\n",
        " validation_data=validation_generator,\r\n",
        " validation_steps=nb_validation_samples//batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 21/755 [..............................] - ETA: 3:59:43 - loss: 2.5837 - accuracy: 0.1855"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98rNPuTvPzQj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "816cc716-163a-4423-f34b-8f7bd427953f"
      },
      "source": [
        "#Now we're going to set up the code for emotion detection\r\n",
        "#Import the libraries\r\n",
        "from keras.models import load_model\r\n",
        "from keras.preprocessing.image import img_to_array\r\n",
        "from keras.preprocessing import image\r\n",
        "import cv2\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "face_classifier=cv2.CascadeClassifier('/haarcascade_frontalface_default.xml')\r\n",
        "classifier = load_model('EmotionDetectionModel.h5')\r\n",
        "\r\n",
        "class_labels=['Angry','Happy','Neutral','Sad','Surprise']\r\n",
        "cap=cv2.VideoCapture(0)\r\n",
        "\r\n",
        "while True:\r\n",
        "    ret,frame=cap.read()\r\n",
        "    labels=[]\r\n",
        "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\r\n",
        "    faces=face_classifier.detectMultiScale(gray,1.3,5)\r\n",
        "\r\n",
        "    for (x,y,w,h) in faces:\r\n",
        "        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,0,0),2)\r\n",
        "        roi_gray=gray[y:y+h,x:x+w]\r\n",
        "        roi_gray=cv2.resize(roi_gray,(48,48),interpolation=cv2.INTER_AREA)\r\n",
        "\r\n",
        "        if np.sum([roi_gray])!=0:\r\n",
        "            roi=roi_gray.astype('float')/255.0\r\n",
        "            roi=img_to_array(roi)\r\n",
        "            roi=np.expand_dims(roi,axis=0)\r\n",
        "\r\n",
        "            preds=classifier.predict(roi)[0]\r\n",
        "            label=class_labels[preds.argmax()]\r\n",
        "            label_position=(x,y)\r\n",
        "            cv2.putText(frame,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\r\n",
        "        else:\r\n",
        "            cv2.putText(frame,'No Face Found',(20,20),cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)\r\n",
        "    \r\n",
        "    cv2.imshow('Emotion Detector',frame)\r\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n",
        "        break\r\n",
        "cap.release()\r\n",
        "cv2.destroyAllWindows() "
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-677211a33c53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mgray\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mfaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    }
  ]
}